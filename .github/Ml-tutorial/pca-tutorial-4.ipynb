{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import Python libraries","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\n# import libraries for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n\n# ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n\n# Working with os module - os is a module in Python 3.\n# Its main purpose is to interact with the operating system. \n# It provides functionalities to manipulate files and folders.\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Check file size","metadata":{}},{"cell_type":"code","source":"print('# File sizes')\nfor f in os.listdir('../input'):\n    print(f.ljust(30) + str(round(os.path.getsize('../input/' + f) / 1000000, 2)) + 'MB')\n","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import dataset","metadata":{}},{"cell_type":"code","source":"%%time\n\nfile = ('../input/adult.csv')\ndf = pd.read_csv(file, encoding='latin-1')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Analysis","metadata":{}},{"cell_type":"markdown","source":"### Check shape of dataset","metadata":{}},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that there are 32561 instances and 15 attributes in the data set.","metadata":{}},{"cell_type":"markdown","source":"### Preview dataset","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### View summary of dataframe","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"Summary of the dataset shows that there are no missing values. But the preview shows that the dataset contains values coded as `?`. So, I will encode `?` as NaN values.","metadata":{}},{"cell_type":"markdown","source":"### Encode `?` as `NaNs`","metadata":{}},{"cell_type":"code","source":"df[df == '?'] = np.nan","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Again check the summary of dataframe","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, the summary shows that the variables - `workclass`, `occupation` and `native.country` contain missing values. All of these variables are categorical data type. So, I will impute the missing values with the most frequent value- the mode.","metadata":{}},{"cell_type":"markdown","source":"### Impute missing values with mode","metadata":{}},{"cell_type":"code","source":"for col in ['workclass', 'occupation', 'native.country']:\n    df[col].fillna(df[col].mode()[0], inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Check again for missing values","metadata":{}},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can see that there are no missing values in the dataset.","metadata":{}},{"cell_type":"markdown","source":"### Setting feature vector and target variable","metadata":{}},{"cell_type":"code","source":"X = df.drop(['income'], axis=1)\n\ny = df['income']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Split data into separate training and test set","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"### Encode categorical variables","metadata":{}},{"cell_type":"code","source":"from sklearn import preprocessing\n\ncategorical = ['workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race', 'sex', 'native.country']\nfor feature in categorical:\n        le = preprocessing.LabelEncoder()\n        X_train[feature] = le.fit_transform(X_train[feature])\n        X_test[feature] = le.transform(X_test[feature])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Scaling","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X.columns)\n\nX_test = pd.DataFrame(scaler.transform(X_test), columns = X.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Logistic Regression model with all features","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\n\nprint('Logistic Regression accuracy score with all the features: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Logistic Regression with PCA\n\nScikit-Learn's PCA class implements PCA algorithm using the code below. Before diving deep, I will explain another important concept called explained variance ratio.\n\n\n### Explained Variance Ratio\n\nA very useful piece of information is the **explained variance ratio** of each principal component. It is available via the `explained_variance_ratio_ ` variable. It indicates the proportion of the datasetâ€™s variance that lies along the axis of each principal component.\n\nNow, let's get to the PCA implementation.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA()\nX_train = pca.fit_transform(X_train)\npca.explained_variance_ratio_\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Comment\n\n- We can see that approximately 97.25% of variance is explained by the first 13 variables. \n\n- Only 2.75% of variance is explained by the last variable. So, we can assume that it carries little information. \n\n- So, I will drop it, train the model again and calculate the accuracy. \n\n","metadata":{}},{"cell_type":"markdown","source":"### Logistic Regression with first 13 features","metadata":{}},{"cell_type":"code","source":"X = df.drop(['income','native.country'], axis=1)\ny = df['income']\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n\n\ncategorical = ['workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race', 'sex']\nfor feature in categorical:\n        le = preprocessing.LabelEncoder()\n        X_train[feature] = le.fit_transform(X_train[feature])\n        X_test[feature] = le.transform(X_test[feature])\n\n\nX_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X.columns)\n\nX_test = pd.DataFrame(scaler.transform(X_test), columns = X.columns)\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\n\nprint('Logistic Regression accuracy score with the first 13 features: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Comment\n\n- We can see that accuracy has been decreased from 0.8218 to 0.8213 after dropping the last feature.\n\n- Now, if I take the last two features combined, then we can see that approximately 7% of variance is explained by them.\n\n- I will drop them, train the model again and calculate the accuracy.\n","metadata":{}},{"cell_type":"markdown","source":"### Logistic Regression with first 12 features","metadata":{}},{"cell_type":"code","source":"X = df.drop(['income','native.country', 'hours.per.week'], axis=1)\ny = df['income']\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n\n\ncategorical = ['workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race', 'sex']\nfor feature in categorical:\n        le = preprocessing.LabelEncoder()\n        X_train[feature] = le.fit_transform(X_train[feature])\n        X_test[feature] = le.transform(X_test[feature])\n\n\nX_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X.columns)\n\nX_test = pd.DataFrame(scaler.transform(X_test), columns = X.columns)\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\n\nprint('Logistic Regression accuracy score with the first 12 features: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Comment\n\n- Now, it can be seen that the accuracy has been increased to 0.8227, if the model is trained with 12 features.\n\n- Lastly, I will take the last three features combined. Approximately 11.83% of variance is explained by them.\n\n- I will repeat the process, drop these features, train the model again and calculate the accuracy.\n","metadata":{}},{"cell_type":"markdown","source":"### Logistic Regression with first 11 features","metadata":{}},{"cell_type":"code","source":"X = df.drop(['income','native.country', 'hours.per.week', 'capital.loss'], axis=1)\ny = df['income']\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n\n\ncategorical = ['workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race', 'sex']\nfor feature in categorical:\n        le = preprocessing.LabelEncoder()\n        X_train[feature] = le.fit_transform(X_train[feature])\n        X_test[feature] = le.transform(X_test[feature])\n\n\nX_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X.columns)\n\nX_test = pd.DataFrame(scaler.transform(X_test), columns = X.columns)\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\n\nprint('Logistic Regression accuracy score with the first 11 features: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Comment\n\n- We can see that accuracy has significantly decreased to 0.8187 if I drop the last three features.\n\n- Our aim is to maximize the accuracy. We get maximum accuracy with the first 12 features and the accuracy is 0.8227.","metadata":{}},{"cell_type":"markdown","source":"## Select right number of dimensions\n\n- The above process works well if the number of dimensions are small.\n\n- But, it is quite cumbersome if we have large number of dimensions.\n\n- In that case, a better approach is to compute the number of dimensions that can explain significantly large portion of the variance.\n\n- The following code computes PCA without reducing dimensionality, then computes the minimum number of dimensions required to preserve 90% of the training set variance.","metadata":{}},{"cell_type":"code","source":"X = df.drop(['income'], axis=1)\ny = df['income']\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n\n\ncategorical = ['workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race', 'sex', 'native.country']\nfor feature in categorical:\n        le = preprocessing.LabelEncoder()\n        X_train[feature] = le.fit_transform(X_train[feature])\n        X_test[feature] = le.transform(X_test[feature])\n\n\nX_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X.columns)\n\n\npca= PCA()\npca.fit(X_train)\ncumsum = np.cumsum(pca.explained_variance_ratio_)\ndim = np.argmax(cumsum >= 0.90) + 1\nprint('The number of dimensions required to preserve 90% of variance is',dim)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Comment\n\n- With the required number of dimensions found, we can then set number of dimensions to `dim` and run PCA again.\n\n- With the number of dimensions set to `dim`, we can then calculate the required accuracy.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8,6))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlim(0,14,1)\nplt.xlabel('Number of components')\nplt.ylabel('Cumulative explained variance')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Comment\n\nThe above plot shows that almost 90% of variance is explained by the first 12 components.","metadata":{}},{"cell_type":"markdown","source":"## Conclusion\n\n-\tIn this kernel, I have discussed Principal Component Analysis â€“ the most popular dimensionality reduction technique.\n-\tI have demonstrated PCA implementation with Logistic Regression on the adult dataset.\n-\tI found the maximum accuracy with the first 12 features and it is found to be 0.8227.\n-\tAs expected, the number of dimensions required to preserve 90 % of variance is found to be 12.\n-\tFinally, I plot the explained variance ratio with number of dimensions. The graph confirms that approximately 90% of variance is explained by the first 12 components.\n","metadata":{"trusted":true}}]}